{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType, ArrayType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import uuid\n",
    "\n",
    "# use in notebook only\n",
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "# Load environment variables from the .env file in the parent directory\n",
    "# dotenv_path = os.path.join(os.path.dirname(__file__), \"..\", \".env\")\n",
    "# load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Cleaning for PostgreSQL Stock Data\n",
    "\n",
    "This PySpark script is designed for data cleaning and schema customization of stock data retrieved from a PostgreSQL database. The code covers the following steps:\n",
    "\n",
    "1. **Setting Up Spark Session:**\n",
    "   - Specifies the path to the PostgreSQL JDBC driver JAR file.\n",
    "   - Creates a Spark session with specific configurations, including the JDBC driver.\n",
    "\n",
    "2. **Reading Database Properties:**\n",
    "   - Reads database connection properties from environment variables such as user, password, driver, and URL.\n",
    "\n",
    "3. **Reading Data from PostgreSQL:**\n",
    "   - Attempts to read stock data from the specified PostgreSQL table using the provided database properties.\n",
    "\n",
    "4. **Defining Custom Schema:**\n",
    "   - Defines a custom schema for the stock data, specifying data types for each column.\n",
    "\n",
    "5. **Applying Custom Schema:**\n",
    "   - Converts the data types of columns in the DataFrame to match the custom schema.\n",
    "\n",
    "6. **Displaying the Result:**\n",
    "   - Shows the cleaned stock data with the custom schema.\n",
    "\n",
    "7. **Exception Handling:**\n",
    "   - Catches and handles any exceptions that may occur during the data reading process, providing detailed error information.\n",
    "\n",
    "Note: Make sure to replace placeholder values such as schema name, table name, and environment variables with actual values specific to your PostgreSQL setup.\n",
    "\n",
    "The code aims to ensure that the stock data adheres to a defined schema for downstream analysis or processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the PostgreSQL JDBC driver JAR file\n",
    "postgres_jar_path = \"./drivers/postgresql-42.6.0.jar\"\n",
    "\n",
    "# Create a Spark session with the PostgreSQL JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockDataCleaning\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars\", postgres_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read database properties from environment variables\n",
    "db_properties = {\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"driver\": os.getenv(\"DB_DRIVER\"),\n",
    "    \"url\": os.getenv(\"DB_URL\"),\n",
    "}\n",
    "\n",
    "# Schema and table name\n",
    "schema_name = \"public\"  # Replace with your actual schema name\n",
    "table_name = f'{schema_name}.\"Stocks\"'\n",
    "\n",
    "# Attempt to read data from PostgreSQL\n",
    "try:\n",
    "    stockData = spark.read.jdbc(url=db_properties[\"url\"],\n",
    "                                table=table_name,\n",
    "                                properties=db_properties)\n",
    "\n",
    "    # Define your custom schema\n",
    "    custom_schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"stock_id\", StringType(), True),\n",
    "        StructField(\"ticker_symbol\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"low\", FloatType(), True),\n",
    "        StructField(\"open\", FloatType(), True),\n",
    "        StructField(\"high\", FloatType(), True),\n",
    "        StructField(\"volume\", IntegerType(), True),\n",
    "        StructField(\"close\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    # Apply the custom schema to the DataFrame\n",
    "    stockData = stockData \\\n",
    "        .withColumn(\"transaction_id\", stockData[\"transaction_id\"].cast(StringType())) \\\n",
    "        .withColumn(\"stock_id\", stockData[\"stock_id\"].cast(StringType())) \\\n",
    "        .withColumn(\"ticker_symbol\", stockData[\"ticker_symbol\"].cast(StringType())) \\\n",
    "        .withColumn(\"date\", stockData[\"date\"].cast(DateType())) \\\n",
    "        .withColumn(\"low\", stockData[\"low\"].cast(FloatType())) \\\n",
    "        .withColumn(\"open\", stockData[\"open\"].cast(FloatType())) \\\n",
    "        .withColumn(\"high\", stockData[\"high\"].cast(FloatType())) \\\n",
    "        .withColumn(\"volume\", stockData[\"volume\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"close\", stockData[\"close\"].cast(FloatType()))\n",
    "\n",
    "    # Show the DataFrame with the custom schema\n",
    "    stockData.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error reading data from PostgreSQL:\")\n",
    "    print(e)\n",
    "    # Print the full stack trace for debugging\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Selecting Latest Stock Data\n",
    "\n",
    "This cell performs data preprocessing on the stock data. It first converts the 'date' column to a date type if it's not already. Then, it creates a window specification for each ticker symbol, ordering the data by date in descending order.\n",
    "\n",
    "A row number is added to the DataFrame based on the window specification, and rows with a row number equal to 1 (representing the latest date) for each group are filtered. The unnecessary 'row_number' column is dropped, and only the essential columns (ticker symbol, date, low, open, high, volume, and close) are selected in the final result.\n",
    "\n",
    "The processed result is displayed using the `show()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to a date type if it's not already\n",
    "stockData = stockData.withColumn(\"date\", F.to_date(stockData[\"date\"]))\n",
    "\n",
    "# Create a window specification for each group, ordered by the 'date' column in descending order\n",
    "windowSpec = Window().partitionBy(\"ticker_symbol\").orderBy(F.desc(\"date\"))\n",
    "\n",
    "# Add a row number to the DataFrame based on the window specification\n",
    "rankedData = stockData.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "\n",
    "# Filter the rows with row number equal to 1 (latest date) for each group\n",
    "latestData = rankedData.filter(\"row_number = 1\").drop(\"row_number\")\n",
    "\n",
    "# Select only the necessary columns\n",
    "result = latestData.select(\"ticker_symbol\", \"date\", \"low\", \"open\", \"high\", \"volume\", \"close\")\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Check: Missing Values\n",
    "\n",
    "This cell performs a check for missing values in the stock data. It calculates the sum of null values for each column and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_values = stockData.select([func.sum(func.col(c).isNull().cast(\"int\")).alias(c + '_missing') for c in stockData.columns]).collect()\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "for row in missing_values[0].asDict():\n",
    "    print(f\"{row}: {missing_values[0][row]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Handling Missing Values and Deduplication\n",
    "\n",
    "This cell addresses data quality by filling missing values in the 'volume' column with 0. Additionally, it removes duplicate rows based on the 'date' and 'close' columns, ensuring data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'volume' column with 0\n",
    "stockData = stockData.na.fill(0, subset=['volume'])\n",
    "\n",
    "# Drop duplicate rows based on 'date' and 'close' columns\n",
    "cleanedStockData = stockData.dropDuplicates(['date', 'close'])\n",
    "\n",
    "# Check for duplicate values in the 'date' column again\n",
    "duplicate_rows = cleanedStockData.groupBy('date', 'close').count().filter('count > 1')\n",
    "\n",
    "# Show the duplicate dates and close prices, if any\n",
    "if duplicate_rows.count() > 0:\n",
    "    print(\"Duplicate dates and close prices found after deduplication:\")\n",
    "    duplicate_rows.show()\n",
    "else:\n",
    "    print(\"No duplicate dates and close prices found.\")\n",
    "\n",
    "cleanedStockData.orderBy(func.desc(\"date\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Views to Update tables otherwise will get errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add the parent directory to the Python path\n",
    "\n",
    "# Create a database connection\n",
    "from database import create_connection, execute_sql\n",
    "\n",
    "connection = create_connection()\n",
    "\n",
    "drop_views = ['DROP MATERIALIZED VIEW IF EXISTS stock_technical_view;']\n",
    "\n",
    "execute_sql(connection, drop_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data Moving Averages Analysis\n",
    "\n",
    "This PySpark script calculates Simple Moving Averages (SMA) and Exponential Moving Averages (EMA) for different periods on stock data. The analysis includes importing libraries, defining functions, setting parameters, and displaying the results.\n",
    "\n",
    "- **Moving Averages:**\n",
    "  - Simple Moving Averages (SMA) for periods: 5, 20, 50, 200.\n",
    "  - Exponential Moving Averages (EMA) with corresponding alpha values.\n",
    "\n",
    "- **Data Manipulation:**\n",
    "  - Utilizes PySpark functions and windows for efficient data processing.\n",
    "\n",
    "- **Result Display:**\n",
    "  - Presents the DataFrame with date, close price, SMAs, and EMAs in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to generate UUIDs\n",
    "@F.udf(StringType())\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "round_to_decimal = 2\n",
    "\n",
    "# Generate a UUID for each row\n",
    "# cleanedStockData = cleanedStockData.withColumn(\"cal_id\", F.lit(str(uuid.uuid4())))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"cal_id\", generate_uuid())\n",
    "\n",
    "def calculate_ema(data, alpha):\n",
    "    ema = data[0]\n",
    "    for i in range(1, len(data)):\n",
    "        ema = alpha * data[i] + (1 - alpha) * ema\n",
    "    return ema\n",
    "\n",
    "calculate_ema_udf = F.udf(lambda data, alpha: float(calculate_ema(data, alpha)), FloatType())\n",
    "\n",
    "periods = [5, 20, 50, 200]\n",
    "alpha_values = [2 / (p + 1) for p in periods]\n",
    "\n",
    "partition_cols = [\"stock_id\", \"ticker_symbol\"]\n",
    "\n",
    "windows = [Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, p - 1) for p in periods]\n",
    "\n",
    "# Calculate simple moving averages\n",
    "for p in periods:\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{p}_days_sma\", F.round(F.avg(\"close\").over(windows[periods.index(p)]), 2))\n",
    "\n",
    "# Calculate exponential moving averages using UDF\n",
    "for p, alpha in zip(periods, alpha_values):\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{p}_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(windows[periods.index(p)]), F.lit(alpha)), round_to_decimal))\n",
    "\n",
    "# Show the result\n",
    "moving_averages_data = cleanedStockData.select(['cal_id','transaction_id',\"stock_id\", \"ticker_symbol\",'date'] + [f\"{p}_days_sma\" for p in periods] + [f\"{p}_days_ema\" for p in periods]).orderBy(F.desc(\"date\"))\n",
    "moving_averages_data.show()\n",
    "\n",
    "# Schema and table name\n",
    "table_name = '\"MovingAverages\"'\n",
    "\n",
    "# Write the DataFrame to the database table\n",
    "moving_averages_data.write.jdbc(os.getenv(\"DB_URL\"), table_name, mode=\"overwrite\", properties=db_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bollinger Bands Calculation Explanation\n",
    "\n",
    "This Jupiter Notebook cell performs the computation of Bollinger Bands on stock data for volatility analysis. The breakdown includes critical steps and considerations:\n",
    "\n",
    "- **Decimal Rounding:**\n",
    "  - All numerical values are rounded to two decimal places for consistency and readability.\n",
    "\n",
    "- **Bollinger Bands Periods:**\n",
    "  - The Bollinger Bands are computed for four distinct periods: 5, 20, 50, and 200 days, providing insights into short-term and long-term volatility.\n",
    "\n",
    "- **Partitioning for Accuracy:**\n",
    "  - The data is partitioned by \"stock_id\" and \"ticker_symbol\" to ensure accurate calculations for individual stocks. This is crucial for meaningful stock market analysis.\n",
    "\n",
    "- **Reuse of Exponential Moving Averages (EMAs):**\n",
    "  - Existing EMA values, previously calculated, are reused in the Bollinger Bands computation. This approach optimizes computational efficiency and maintains consistency with prior analyses.\n",
    "\n",
    "- **Upper and Lower Band Calculation:**\n",
    "  - The upper and lower bands are determined by adding and subtracting twice the standard deviation of closing prices from the corresponding EMAs. This methodology aligns with the standard Bollinger Bands formula.\n",
    "\n",
    "- **Result Presentation:**\n",
    "  - The final DataFrame includes the date, close price, upper bands, and lower bands for each specified period, providing a comprehensive view of the stock's volatility.\n",
    "\n",
    "This code enhances the dataset with Bollinger Bands, aiding in the identification of potential market trends and volatility patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bollinger Bands periods\n",
    "bollinger_periods = [5, 20, 50, 200]\n",
    "\n",
    "partition_cols = [\"stock_id\", \"ticker_symbol\"]\n",
    "\n",
    "# Define the windows for Bollinger Bands\n",
    "windows = [Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, p - 1) for p in bollinger_periods]\n",
    "\n",
    "# Reuse the existing EMA values for Bollinger Bands\n",
    "for p in bollinger_periods:\n",
    "    upper_band_col = F.col(f\"{p}_days_ema\") + (2 * F.stddev(\"close\").over(windows[bollinger_periods.index(p)]))\n",
    "    lower_band_col = F.col(f\"{p}_days_ema\") - (2 * F.stddev(\"close\").over(windows[bollinger_periods.index(p)]))\n",
    "\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{p}_upper_band\", F.round(upper_band_col, round_to_decimal))\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{p}_lower_band\", F.round(lower_band_col, round_to_decimal))\n",
    "\n",
    "# Show the result\n",
    "selected_columns = ['cal_id','transaction_id',\"stock_id\", \"ticker_symbol\",'date'] + [f\"{p}_upper_band\" for p in bollinger_periods] + [f\"{p}_lower_band\" for p in bollinger_periods]\n",
    "boilling_bands_data = cleanedStockData.select(selected_columns).orderBy(F.desc(\"date\"))\n",
    "boilling_bands_data.show()\n",
    "\n",
    "# Schema and table name\n",
    "table_name = '\"BoillingerBands\"'\n",
    "\n",
    "# Write the DataFrame to the database table\n",
    "boilling_bands_data.write.jdbc(os.getenv(\"DB_URL\"), table_name, mode=\"overwrite\", properties=db_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Strength Index (RSI) Calculation Explanation\n",
    "\n",
    "This code cell calculates the Relative Strength Index (RSI) for a given stock dataset. The RSI is a momentum oscillator that measures the speed and change of price movements. The breakdown includes critical steps and considerations:\n",
    "\n",
    "- **Price Changes:**\n",
    "  - The code calculates the daily price changes by subtracting the previous day's closing price from the current day's closing price.\n",
    "\n",
    "- **Gains and Losses:**\n",
    "  - Gains and losses are determined based on whether the price change is positive or negative.\n",
    "\n",
    "- **Exponential Moving Averages (EMAs):**\n",
    "  - Exponential Moving Averages (EMAs) are calculated for both gains and losses over the specified RSI period. EMAs give more weight to recent price changes, providing a more responsive indicator.\n",
    "\n",
    "- **Handling NULL and Zero Values:**\n",
    "  - NULL values for average gains are handled by replacing them with zero. Additionally, zero values for average losses are replaced with zero to avoid division errors.\n",
    "\n",
    "- **Relative Strength (RS) Calculation:**\n",
    "  - The Relative Strength (RS) is calculated as the ratio of average gains to average losses.\n",
    "\n",
    "- **RSI Calculation:**\n",
    "  - The RSI is calculated using the standard formula: \\(100 - \\frac{100}{1 + RS}\\). This value is then rounded to two decimal places.\n",
    "\n",
    "- **List of RSI Periods:**\n",
    "  - The code calculates RSI for multiple periods, including 14, 20, 50, and 200 days, providing insights into short-term and long-term momentum.\n",
    "\n",
    "- **Result Presentation:**\n",
    "  - The final DataFrame includes columns for the ticker symbol, date, closing price, and RSI for each specified period, offering a comprehensive view of the stock's momentum trends.\n",
    "\n",
    "- **Data Partitioning:**\n",
    "  - The data is partitioned by \"stock_id\" and \"ticker_symbol\" to ensure accurate calculations for individual stocks. This is crucial for meaningful stock market analysis.\n",
    "\n",
    "- **Decimal Rounding:**\n",
    "  - All numerical values, including the calculated RSI, are rounded to two decimal places for consistency and readability.\n",
    "\n",
    "This code enhances the dataset with RSI values, contributing to the analysis of potential overbought or oversold market conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, n):\n",
    "    # Calculate price changes\n",
    "    price_diff = F.col(\"close\") - F.lag(\"close\", 1).over(Window().partitionBy(\"stock_id\", \"ticker_symbol\").orderBy(\"date\"))\n",
    "    \n",
    "    # Separate gains and losses\n",
    "    gains = F.when(price_diff > 0, price_diff).otherwise(0)\n",
    "    losses = F.when(price_diff < 0, -price_diff).otherwise(0)\n",
    "    \n",
    "    # Calculate average gains and losses over n periods from the latest day backward\n",
    "    avg_gains = F.avg(gains).over(Window().partitionBy(\"stock_id\", \"ticker_symbol\").orderBy(F.desc(\"date\")).rowsBetween(0, n-1))\n",
    "    avg_losses = F.avg(losses).over(Window().partitionBy(\"stock_id\", \"ticker_symbol\").orderBy(F.desc(\"date\")).rowsBetween(0, n-1))\n",
    "    \n",
    "    # Handle NULL values for average gains\n",
    "    avg_gains = F.coalesce(avg_gains, F.lit(0))\n",
    "\n",
    "    # Handle 0 values for average losses\n",
    "    avg_losses = F.when(avg_losses.isNull() | (avg_losses == 0), 0).otherwise(avg_losses)\n",
    "\n",
    "    # Calculate RSI\n",
    "    rs = F.when((avg_losses == 0) & (avg_gains != 0), F.lit(avg_gains)) \\\n",
    "      .when((avg_losses != 0) & (avg_gains == 0), F.lit(0)) \\\n",
    "      .otherwise(F.when(avg_losses == 0, F.lit(float('inf'))) \\\n",
    "                  .otherwise(avg_gains / avg_losses))\n",
    "    \n",
    "    # Calculate RSI and round to 2 decimal places\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return F.round(rsi, round_to_decimal)\n",
    "\n",
    "# List of RSI periods\n",
    "rsi_periods = [14, 20, 50, 200]\n",
    "\n",
    "# Calculate and add RSI columns to the DataFrame for each period\n",
    "for n in rsi_periods:\n",
    "    column_name = f\"{n}_days_rsi\"\n",
    "    cleanedStockData = cleanedStockData.withColumn(column_name, calculate_rsi(cleanedStockData, n))\n",
    "\n",
    "# Show the result\n",
    "result_columns = ['cal_id','transaction_id',\"stock_id\", \"ticker_symbol\",'date'] + [f\"{n}_days_rsi\" for n in rsi_periods]\n",
    "relative_indexes_data = cleanedStockData.select(result_columns).orderBy(F.desc(\"date\"))\n",
    "relative_indexes_data.show()\n",
    "\n",
    "# Schema and table name\n",
    "table_name = '\"RelativeIndexes\"'\n",
    "\n",
    "relative_indexes_data.write.jdbc(os.getenv(\"DB_URL\"), table_name, mode=\"overwrite\", properties=db_properties)\n",
    "\n",
    "# Stop the Spark session when you're done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update tables to original structures after overwriten process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alter tables to approriate data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_statements = [\n",
    "    # Altered data types for MovingAverages table\n",
    "    \"\"\"\n",
    "    ALTER TABLE \"MovingAverages\"\n",
    "    ALTER COLUMN \"cal_id\" TYPE UUID USING \"transaction_id\"::UUID,\n",
    "    ALTER COLUMN \"transaction_id\" TYPE UUID USING \"transaction_id\"::UUID,\n",
    "    ALTER COLUMN \"stock_id\" TYPE UUID USING \"stock_id\"::UUID,\n",
    "    ALTER COLUMN \"ticker_symbol\" TYPE VARCHAR,\n",
    "    ALTER COLUMN \"date\" TYPE DATE,\n",
    "    ALTER COLUMN \"5_days_sma\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"20_days_sma\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"50_days_sma\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"200_days_sma\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"5_days_ema\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"20_days_ema\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"50_days_ema\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"200_days_ema\" TYPE FLOAT;\n",
    "    \"\"\",\n",
    "    # Altered data types for BoillingerBands table\n",
    "    \"\"\"\n",
    "    ALTER TABLE \"BoillingerBands\"\n",
    "    ALTER COLUMN \"cal_id\" TYPE UUID USING \"transaction_id\"::UUID,\n",
    "    ALTER COLUMN \"transaction_id\" TYPE UUID USING \"transaction_id\"::UUID,\n",
    "    ALTER COLUMN \"stock_id\" TYPE UUID USING \"stock_id\"::UUID,\n",
    "    ALTER COLUMN \"ticker_symbol\" TYPE VARCHAR,\n",
    "    ALTER COLUMN \"date\" TYPE DATE,\n",
    "    ALTER COLUMN \"5_upper_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"20_upper_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"50_upper_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"200_upper_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"5_lower_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"20_lower_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"50_lower_band\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"200_lower_band\" TYPE FLOAT;\n",
    "    \"\"\",\n",
    "\n",
    "    # Altered data types for RelativeIndexes table\n",
    "    \"\"\"\n",
    "    ALTER TABLE \"RelativeIndexes\"\n",
    "    ALTER COLUMN \"cal_id\" TYPE UUID USING \"transaction_id\"::UUID,\n",
    "    ALTER COLUMN \"transaction_id\" TYPE UUID USING \"transaction_id\"::UUID,\n",
    "    ALTER COLUMN \"stock_id\" TYPE UUID USING \"stock_id\"::UUID,\n",
    "    ALTER COLUMN \"ticker_symbol\" TYPE VARCHAR,\n",
    "    ALTER COLUMN \"date\" TYPE DATE,\n",
    "    ALTER COLUMN \"14_days_rsi\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"20_days_rsi\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"50_days_rsi\" TYPE FLOAT,\n",
    "    ALTER COLUMN \"200_days_rsi\" TYPE FLOAT;\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute SQL statements\n",
    "execute_sql(connection, alter_statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding key constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_key_constrains_statements = [\n",
    "    # MovingAverages\n",
    "    'ALTER TABLE \"MovingAverages\" ADD CONSTRAINT \"pk_MovingAverages_cal_id\" PRIMARY KEY (\"cal_id\");',\n",
    "    'ALTER TABLE \"MovingAverages\" ADD CONSTRAINT \"fk_MovingAverages_transaction_id\" FOREIGN KEY (\"transaction_id\") REFERENCES \"Stocks\"(\"transaction_id\") ON DELETE CASCADE;',\n",
    "    'ALTER TABLE \"MovingAverages\" ADD CONSTRAINT \"fk_MovingAverages_stock_id_ticker_symbol\" FOREIGN KEY (\"stock_id\", \"ticker_symbol\") REFERENCES \"CompanyInformation\"(\"stock_id\", \"ticker_symbol\");',\n",
    "\n",
    "    # BoillingerBands\n",
    "    'ALTER TABLE \"BoillingerBands\" ADD CONSTRAINT \"pk_BoillingerBands_cal_id\" PRIMARY KEY (\"cal_id\");',\n",
    "    'ALTER TABLE \"BoillingerBands\" ADD CONSTRAINT \"fk_BoillingerBands_transaction_id\" FOREIGN KEY (\"transaction_id\") REFERENCES \"Stocks\"(\"transaction_id\") ON DELETE CASCADE;',\n",
    "    'ALTER TABLE \"BoillingerBands\" ADD CONSTRAINT \"fk_BoillingerBands_stock_id_ticker_symbol\" FOREIGN KEY (\"stock_id\", \"ticker_symbol\") REFERENCES \"CompanyInformation\"(\"stock_id\", \"ticker_symbol\");',\n",
    "\n",
    "    # RelativeIndexes\n",
    "    'ALTER TABLE \"RelativeIndexes\" ADD CONSTRAINT \"pk_RelativeIndexes_cal_id\" PRIMARY KEY (\"cal_id\");',\n",
    "    'ALTER TABLE \"RelativeIndexes\" ADD CONSTRAINT \"fk_RelativeIndexes_transaction_id\" FOREIGN KEY (\"transaction_id\") REFERENCES \"Stocks\"(\"transaction_id\") ON DELETE CASCADE;',\n",
    "    'ALTER TABLE \"RelativeIndexes\" ADD CONSTRAINT \"fk_RelativeIndexes_stock_id_ticker_symbol\" FOREIGN KEY (\"stock_id\", \"ticker_symbol\") REFERENCES \"CompanyInformation\"(\"stock_id\", \"ticker_symbol\");'\n",
    "]\n",
    "\n",
    "# Execute SQL statements\n",
    "execute_sql(connection, alter_key_constrains_statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SQL statements for indexes\n",
    "index_sql_statements = [\n",
    "    'CREATE INDEX IF NOT EXISTS idx_ma_stock_id_ticker_symbol_date ON \"MovingAverages\"(\"stock_id\", \"ticker_symbol\", \"date\");',\n",
    "    'CREATE INDEX IF NOT EXISTS idx_ma_date ON \"MovingAverages\"(\"date\");',\n",
    "    'CREATE INDEX IF NOT EXISTS idx_bb_stock_id_ticker_symbol_date ON \"BoillingerBands\"(\"stock_id\", \"ticker_symbol\", \"date\");',\n",
    "    'CREATE INDEX IF NOT EXISTS idx_bb_date ON \"BoillingerBands\"(\"date\");',\n",
    "    'CREATE INDEX IF NOT EXISTS idx_ri_stock_id_ticker_symbol_date ON \"RelativeIndexes\"(\"stock_id\", \"ticker_symbol\", \"date\");',\n",
    "    'CREATE INDEX IF NOT EXISTS idx_ri_date ON \"RelativeIndexes\"(\"date\");',\n",
    "]\n",
    "\n",
    "# Execute SQL statements\n",
    "execute_sql(connection, index_sql_statements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
