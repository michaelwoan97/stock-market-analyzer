{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType, ArrayType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/13 19:56:00 WARN Utils: Your hostname, michaelwoan-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/11/13 19:56:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/13 19:56:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/13 19:56:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"StockDataCleaning\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"transaction_id\", StringType(), True), \\\n",
    "    StructField(\"stock_id\", StringType(), True), \\\n",
    "    StructField(\"ticker_symbol\", StringType(), True), \\\n",
    "    StructField(\"date\", DateType(), True), \\\n",
    "    StructField(\"low\", FloatType(), True), \\\n",
    "    StructField(\"open\", FloatType(), True), \\\n",
    "    StructField(\"high\", FloatType(), True), \\\n",
    "    StructField(\"volume\", IntegerType(), True), \\\n",
    "    StructField(\"close\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockData = spark.read.schema(schema).csv(\"./stocks-data/combined-stocks-data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- ticker_symbol: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stockData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+---------+--------+\n",
      "|      transaction_id|            stock_id|ticker_symbol|      date|     low|    open|    high|   volume|   close|\n",
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+---------+--------+\n",
      "|b91319ee-7e00-422...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|c25c9b76-bba2-42d...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|eb207f83-6265-4e6...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|4b83d021-0fc5-465...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|06a5cd4a-53ff-49b...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|9fc036ba-1f88-48c...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|c2132b60-d0d2-4ba...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|b27aaf74-c27d-45e...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|5d8f3f36-763c-482...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|e393206f-11a0-4a6...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|9db51124-4c78-4b5...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|9961d5f3-512b-4c4...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|2a4a171e-006d-4a5...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|51c5e3e3-1a53-429...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|949e1e98-eb64-46e...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|71506f8c-825c-41e...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|5ecad87d-a5ba-46d...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "|e719d20b-3784-42c...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "|eafc31c8-9a21-485...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "|464a4bf8-6690-474...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stockData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================>                                   (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "transaction_id_missing: 0\n",
      "stock_id_missing: 0\n",
      "ticker_symbol_missing: 0\n",
      "date_missing: 0\n",
      "low_missing: 0\n",
      "open_missing: 0\n",
      "high_missing: 0\n",
      "volume_missing: 140\n",
      "close_missing: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_values = stockData.select([func.sum(func.col(c).isNull().cast(\"int\")).alias(c + '_missing') for c in stockData.columns]).collect()\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "for row in missing_values[0].asDict():\n",
    "    print(f\"{row}: {missing_values[0][row]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+------+--------+\n",
      "|      transaction_id|            stock_id|ticker_symbol|      date|     low|    open|    high|volume|   close|\n",
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+------+--------+\n",
      "|56bd39f1-60c3-41a...|c5e95634-4871-49f...|         AAPL|1983-09-23| 0.09933|0.111607|0.111607|  NULL|0.108259|\n",
      "|869ea8de-1017-40f...|c5e95634-4871-49f...|         AAPL|1983-09-23| 0.09933|0.111607|0.111607|  NULL|0.108259|\n",
      "|f1a92d79-219f-424...|c5e95634-4871-49f...|         AAPL|1983-09-23| 0.09933|0.111607|0.111607|  NULL|0.108259|\n",
      "|8829ded0-09be-431...|c5e95634-4871-49f...|         AAPL|1983-09-23| 0.09933|0.111607|0.111607|  NULL|0.108259|\n",
      "|1f356f8d-6d7b-4d8...|c5e95634-4871-49f...|         AAPL|1997-08-06|0.223214|0.225446|0.247768|  NULL|0.234933|\n",
      "|8b6b3e2b-f50d-4b6...|c5e95634-4871-49f...|         AAPL|1997-08-06|0.223214|0.225446|0.247768|  NULL|0.234933|\n",
      "|922c8d07-ea2d-44f...|c5e95634-4871-49f...|         AAPL|1997-08-06|0.223214|0.225446|0.247768|  NULL|0.234933|\n",
      "|7dab6a41-0d18-4fc...|c5e95634-4871-49f...|         AAPL|1997-08-06|0.223214|0.225446|0.247768|  NULL|0.234933|\n",
      "|d59ef71f-de07-4c0...|c5e95634-4871-49f...|         AAPL|1997-08-07|0.253348|0.256696|0.263951|  NULL|0.260603|\n",
      "|b1c1bf12-95e4-4c0...|c5e95634-4871-49f...|         AAPL|1997-08-07|0.253348|0.256696|0.263951|  NULL|0.260603|\n",
      "|e75e76c7-c548-49e...|c5e95634-4871-49f...|         AAPL|1997-08-07|0.253348|0.256696|0.263951|  NULL|0.260603|\n",
      "|1e983905-5fd8-4c1...|c5e95634-4871-49f...|         AAPL|1997-08-07|0.253348|0.256696|0.263951|  NULL|0.260603|\n",
      "|834d0f68-159e-4cf...|c5e95634-4871-49f...|         AAPL|1998-07-16|0.319196| 0.33817|0.340402|  NULL|0.334821|\n",
      "|ba574439-fe57-4b5...|c5e95634-4871-49f...|         AAPL|1998-07-16|0.319196| 0.33817|0.340402|  NULL|0.334821|\n",
      "|960c7cbe-0de4-41e...|c5e95634-4871-49f...|         AAPL|1998-07-16|0.319196| 0.33817|0.340402|  NULL|0.334821|\n",
      "|9528236a-594b-428...|c5e95634-4871-49f...|         AAPL|1998-07-16|0.319196| 0.33817|0.340402|  NULL|0.334821|\n",
      "|f19bc895-27a3-421...|c5e95634-4871-49f...|         AAPL|1998-10-14|0.328683|0.354911|0.368862|  NULL|0.333705|\n",
      "|fe4b448e-ef8b-435...|c5e95634-4871-49f...|         AAPL|1998-10-14|0.328683|0.354911|0.368862|  NULL|0.333705|\n",
      "|2546c3e3-96b6-4ac...|c5e95634-4871-49f...|         AAPL|1998-10-14|0.328683|0.354911|0.368862|  NULL|0.333705|\n",
      "|e72328ef-8992-463...|c5e95634-4871-49f...|         AAPL|1998-10-14|0.328683|0.354911|0.368862|  NULL|0.333705|\n",
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'volume' is missing\n",
    "missing_volume_rows = stockData.filter(stockData['volume'].isNull())\n",
    "\n",
    "# Show the missing values\n",
    "missing_volume_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+---------+--------+\n",
      "|      transaction_id|            stock_id|ticker_symbol|      date|     low|    open|    high|   volume|   close|\n",
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+---------+--------+\n",
      "|b91319ee-7e00-422...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|c25c9b76-bba2-42d...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|eb207f83-6265-4e6...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|4b83d021-0fc5-465...|c1fe7393-9e77-407...|         TSLA|2010-06-29|1.169333|1.266667|1.666667|281494500|1.592667|\n",
      "|06a5cd4a-53ff-49b...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|9fc036ba-1f88-48c...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|c2132b60-d0d2-4ba...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|b27aaf74-c27d-45e...|c1fe7393-9e77-407...|         TSLA|2010-06-30|1.553333|1.719333|   2.028|257806500|1.588667|\n",
      "|5d8f3f36-763c-482...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|e393206f-11a0-4a6...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|9db51124-4c78-4b5...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|9961d5f3-512b-4c4...|c1fe7393-9e77-407...|         TSLA|2010-07-01|1.351333|1.666667|   1.728|123282000|   1.464|\n",
      "|2a4a171e-006d-4a5...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|51c5e3e3-1a53-429...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|949e1e98-eb64-46e...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|71506f8c-825c-41e...|c1fe7393-9e77-407...|         TSLA|2010-07-02|1.247333|1.533333|    1.54| 77097000|    1.28|\n",
      "|5ecad87d-a5ba-46d...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "|e719d20b-3784-42c...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "|eafc31c8-9a21-485...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "|464a4bf8-6690-474...|c1fe7393-9e77-407...|         TSLA|2010-07-06|1.055333|1.333333|1.333333|103003500|   1.074|\n",
      "+--------------------+--------------------+-------------+----------+--------+--------+--------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in the 'volume' column with 0\n",
    "stockData = stockData.na.fill(0, subset=['volume'])\n",
    "\n",
    "# Show the DataFrame after filling missing values\n",
    "stockData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate dates and close prices found.\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate rows based on 'date' and 'close' columns\n",
    "cleanedStockData = stockData.dropDuplicates(['date', 'close'])\n",
    "\n",
    "# Check for duplicate values in the 'date' column again\n",
    "duplicate_rows = cleanedStockData.groupBy('date', 'close').count().filter('count > 1')\n",
    "\n",
    "# Show the duplicate dates and close prices, if any\n",
    "if duplicate_rows.count() > 0:\n",
    "    print(\"Duplicate dates and close prices found after deduplication:\")\n",
    "    duplicate_rows.show()\n",
    "else:\n",
    "    print(\"No duplicate dates and close prices found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+----------+--------+------+------+---------+------+\n",
      "|      transaction_id|            stock_id|ticker_symbol|      date|     low|  open|  high|   volume| close|\n",
      "+--------------------+--------------------+-------------+----------+--------+------+------+---------+------+\n",
      "|f38ed296-ff46-4c1...|0cb4048a-7fe6-441...|         GOOG|2023-11-13|  132.77|133.36|134.11| 14786236|133.64|\n",
      "|8cbdb4d4-889d-487...|c5e95634-4871-49f...|         AAPL|2023-11-13|  184.21|185.82|186.03| 43553315| 184.8|\n",
      "|50787f93-b075-47c...|c1fe7393-9e77-407...|         TSLA|2023-11-13|211.6101| 215.6| 225.4|137561292|223.71|\n",
      "|113f51fd-6da3-4cd...|1ec4324e-7d11-49b...|         MSFT|2023-11-13| 365.915|368.22|368.46| 19318985|366.68|\n",
      "|bca35b47-8911-453...|0cb4048a-7fe6-441...|         GOOG|2023-11-10|  130.87|131.53|134.27| 20872900|134.06|\n",
      "|f8a125fa-728b-4b0...|c5e95634-4871-49f...|         AAPL|2023-11-10|  183.53|183.97|186.57| 66133400| 186.4|\n",
      "|aae8bec2-423c-4c2...|c1fe7393-9e77-407...|         TSLA|2023-11-10|  205.69|210.03|215.38|130994000|214.65|\n",
      "|c1ca9f53-f3d2-4a5...|1ec4324e-7d11-49b...|         MSFT|2023-11-10|  361.07|361.49| 370.1| 28042100|369.67|\n",
      "|49d3663a-6be6-461...|c5e95634-4871-49f...|         AAPL|2023-11-09|  181.81|182.96|184.12| 53763500|182.41|\n",
      "|b7eea92e-5412-444...|0cb4048a-7fe6-441...|         GOOG|2023-11-09|  131.51|133.36|133.96| 17976500|131.69|\n",
      "|0381630b-9762-44f...|c1fe7393-9e77-407...|         TSLA|2023-11-09|  206.68|219.75| 220.8|142110500|209.98|\n",
      "|6843a37e-5b4a-48a...|1ec4324e-7d11-49b...|         MSFT|2023-11-09|  360.36| 362.3|364.79| 24847300|360.69|\n",
      "|31ee26ac-5f8a-449...|0cb4048a-7fe6-441...|         GOOG|2023-11-08|  132.16|132.36|133.54| 15093600|133.26|\n",
      "|3b114048-ac15-41e...|c5e95634-4871-49f...|         AAPL|2023-11-08|  181.59|182.35|183.45| 49340300|182.89|\n",
      "|88900b04-c036-438...|c1fe7393-9e77-407...|         TSLA|2023-11-08|  217.64|223.15|224.15|106584800|222.11|\n",
      "|0f50336c-d3e1-49d...|1ec4324e-7d11-49b...|         MSFT|2023-11-08|  360.55|361.68|363.87| 26767800| 363.2|\n",
      "|198e08d1-16af-4d7...|c1fe7393-9e77-407...|         TSLA|2023-11-07|  215.72|219.98|223.12|116900100|222.18|\n",
      "|2f86468b-4d2f-4b7...|0cb4048a-7fe6-441...|         GOOG|2023-11-07|  131.14|131.98|133.28| 19223800| 132.4|\n",
      "|e359f53f-88b8-4ac...|c5e95634-4871-49f...|         AAPL|2023-11-07|  178.97|179.18|182.44| 70530000|181.82|\n",
      "|814174c1-19a6-4c8...|1ec4324e-7d11-49b...|         MSFT|2023-11-07|  357.63| 359.4|362.46| 25833900|360.53|\n",
      "+--------------------+--------------------+-------------+----------+--------+------+------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleanedStockData.orderBy(func.desc(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/13 20:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/13 20:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/13 20:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/13 20:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/13 20:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/13 20:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-----------+-----------+------------+----------+-----------+-----------+------------+\n",
      "|      date| close|5_days_sma|20_days_sma|50_days_sma|200_days_sma|5_days_ema|20_days_ema|50_days_ema|200_days_ema|\n",
      "+----------+------+----------+-----------+-----------+------------+----------+-----------+-----------+------------+\n",
      "|2023-11-13|366.68|    366.68|     366.68|     366.68|      366.68|    221.31|     224.78|     225.42|       242.4|\n",
      "|2023-11-13|223.71|    223.71|     223.71|     223.71|      223.71|    209.45|     222.65|     224.57|      222.83|\n",
      "|2023-11-13| 184.8|     184.8|      184.8|      184.8|       184.8|    219.45|     226.15|     225.99|      217.85|\n",
      "|2023-11-13|133.64|    133.64|     133.64|     133.64|      133.64|    260.78|     235.55|     229.61|      211.67|\n",
      "|2023-11-10|214.65|    214.65|     214.65|     214.65|      214.65|    207.41|     221.83|     224.32|      221.65|\n",
      "|2023-11-10|134.06|    134.06|     134.06|     134.06|      134.06|    257.09|     234.68|     229.39|      211.78|\n",
      "|2023-11-10|369.67|    369.67|     369.67|     369.67|      369.67|     224.3|     225.28|      225.7|      242.99|\n",
      "|2023-11-10| 186.4|     186.4|      186.4|      186.4|       186.4|    217.18|     225.41|      225.8|      218.12|\n",
      "|2023-11-09|209.98|    209.98|     209.98|     209.98|      209.98|    207.99|     222.08|     224.51|      221.18|\n",
      "|2023-11-09|182.41|    182.41|     182.41|     182.41|      182.41|    218.17|      225.7|      226.0|      217.75|\n",
      "|2023-11-09|360.69|    360.69|     360.69|     360.69|      360.69|     224.3|     225.39|     225.83|      241.93|\n",
      "|2023-11-09|131.69|    131.69|     131.69|     131.69|      131.69|    258.79|     235.04|     229.62|      211.62|\n",
      "|2023-11-08| 363.2|     363.2|      363.2|      363.2|       363.2|    223.49|     225.15|      225.8|      242.37|\n",
      "|2023-11-08|133.26|    133.26|     133.26|     133.26|      133.26|    259.13|     235.24|     229.79|      212.01|\n",
      "|2023-11-08|182.89|    182.89|     182.89|     182.89|      182.89|    217.92|     225.74|     226.11|      217.97|\n",
      "|2023-11-08|222.11|    222.11|     222.11|     222.11|      222.11|    206.65|     221.78|     224.48|      222.94|\n",
      "|2023-11-07|222.18|    222.18|     222.18|     222.18|      222.18|    204.22|     220.87|     224.15|      222.97|\n",
      "|2023-11-07| 132.4|     132.4|      132.4|      132.4|       132.4|    258.47|     234.98|     229.75|       212.0|\n",
      "|2023-11-07|360.53|    360.53|     360.53|     360.53|      360.53|    215.76|     222.58|     224.77|      241.85|\n",
      "|2023-11-07|181.82|    181.82|     181.82|     181.82|      181.82|    216.24|     225.15|     225.93|      217.89|\n",
      "+----------+------+----------+-----------+-----------+------------+----------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Updated function to calculate exponential moving average\n",
    "def calculate_ema(data, alpha):\n",
    "    ema = data[0]  # Set the initial value as the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        ema = alpha * data[i] + (1 - alpha) * ema\n",
    "    \n",
    "    return ema\n",
    "\n",
    "# Convert the calculate_ema_udf function to a Spark UDF\n",
    "calculate_ema_udf = F.udf(lambda data, alpha: float(calculate_ema(data, alpha)), FloatType())\n",
    "\n",
    "# Define the window specifications for different moving averages\n",
    "short_term_window = Window().partitionBy(\"transaction_id\", \"stock_id\").orderBy(F.desc(\"date\")).rowsBetween(0, 4)\n",
    "intermediate_term_window = Window().partitionBy(\"transaction_id\", \"stock_id\").orderBy(F.desc(\"date\")).rowsBetween(0, 19)\n",
    "fifty_day_window = Window().partitionBy(\"transaction_id\", \"stock_id\").orderBy(F.desc(\"date\")).rowsBetween(0, 49)\n",
    "long_term_window = Window().partitionBy(\"transaction_id\", \"stock_id\").orderBy(F.desc(\"date\")).rowsBetween(0, 199)\n",
    "\n",
    "# Calculate moving averages for each window\n",
    "cleanedStockData = cleanedStockData.withColumn(\"5_days_sma\", F.round(F.avg(\"close\").over(short_term_window), 2))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_sma\", F.round(F.avg(\"close\").over(intermediate_term_window), 2))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"50_days_sma\", F.round(F.avg(\"close\").over(fifty_day_window), 2))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"200_days_sma\", F.round(F.avg(\"close\").over(long_term_window), 2))\n",
    "\n",
    "# Define the periods and alpha values for exponential moving averages\n",
    "short_term_period = 5\n",
    "intermediate_term_period = 20\n",
    "fifty_day_period = 50\n",
    "long_term_period = 200\n",
    "alpha_short = 2 / (short_term_period + 1)\n",
    "alpha_intermediate = 2 / (intermediate_term_period + 1)\n",
    "alpha_fifty = 2 / (fifty_day_period + 1)\n",
    "alpha_long = 2 / (long_term_period + 1)\n",
    "\n",
    "# Apply the UDF to calculate EMAs\n",
    "cleanedStockData = cleanedStockData.withColumn(\"5_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(Window().orderBy(F.desc(\"date\")).rowsBetween(0, short_term_period - 1)), F.lit(alpha_short)), 2))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(Window().orderBy(F.desc(\"date\")).rowsBetween(0, intermediate_term_period - 1)), F.lit(alpha_intermediate)), 2))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"50_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(Window().orderBy(F.desc(\"date\")).rowsBetween(0, fifty_day_period - 1)), F.lit(alpha_fifty)), 2))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"200_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(Window().orderBy(F.desc(\"date\")).rowsBetween(0, long_term_period - 1)), F.lit(alpha_long)), 2))\n",
    "\n",
    "# Show the result\n",
    "cleanedStockData.select(['date', 'close', '5_days_sma', '20_days_sma', '50_days_sma', '200_days_sma', '5_days_ema', '20_days_ema', '50_days_ema', '200_days_ema']).orderBy(F.desc(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+-----------+-----------+------------+\n",
      "|ticker_symbol|      date| close|5_days_sma|20_days_sma|50_days_sma|200_days_sma|\n",
      "+-------------+----------+------+----------+-----------+-----------+------------+\n",
      "|         MSFT|2023-11-13|366.68|    366.68|     366.68|     366.68|      366.68|\n",
      "|         TSLA|2023-11-13|223.71|    223.71|     223.71|     223.71|      223.71|\n",
      "|         AAPL|2023-11-13| 184.8|     184.8|      184.8|      184.8|       184.8|\n",
      "|         GOOG|2023-11-13|133.64|    133.64|     133.64|     133.64|      133.64|\n",
      "|         TSLA|2023-11-10|214.65|    214.65|     214.65|     214.65|      214.65|\n",
      "|         GOOG|2023-11-10|134.06|    134.06|     134.06|     134.06|      134.06|\n",
      "|         MSFT|2023-11-10|369.67|    369.67|     369.67|     369.67|      369.67|\n",
      "|         AAPL|2023-11-10| 186.4|     186.4|      186.4|      186.4|       186.4|\n",
      "|         TSLA|2023-11-09|209.98|    209.98|     209.98|     209.98|      209.98|\n",
      "|         AAPL|2023-11-09|182.41|    182.41|     182.41|     182.41|      182.41|\n",
      "|         MSFT|2023-11-09|360.69|    360.69|     360.69|     360.69|      360.69|\n",
      "|         GOOG|2023-11-09|131.69|    131.69|     131.69|     131.69|      131.69|\n",
      "|         MSFT|2023-11-08| 363.2|     363.2|      363.2|      363.2|       363.2|\n",
      "|         GOOG|2023-11-08|133.26|    133.26|     133.26|     133.26|      133.26|\n",
      "|         AAPL|2023-11-08|182.89|    182.89|     182.89|     182.89|      182.89|\n",
      "|         TSLA|2023-11-08|222.11|    222.11|     222.11|     222.11|      222.11|\n",
      "|         TSLA|2023-11-07|222.18|    222.18|     222.18|     222.18|      222.18|\n",
      "|         GOOG|2023-11-07| 132.4|     132.4|      132.4|      132.4|       132.4|\n",
      "|         MSFT|2023-11-07|360.53|    360.53|     360.53|     360.53|      360.53|\n",
      "|         AAPL|2023-11-07|181.82|    181.82|     181.82|     181.82|      181.82|\n",
      "+-------------+----------+------+----------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# minus 1 cuz of 0-based index\n",
    "short_term_date = 4\n",
    "intermediate_term_date = 19\n",
    "long_term_date = 199\n",
    "fifty_day_date = 49\n",
    "round_to_decimal = 2\n",
    "\n",
    "partition_cols = [\"transaction_id\", \"stock_id\"]\n",
    "\n",
    "# Define the window specifications for different moving averages\n",
    "short_term_window = Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, short_term_date)\n",
    "intermediate_term_window = Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, intermediate_term_date)\n",
    "fifty_day_window = Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, fifty_day_date)\n",
    "long_term_window = Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, long_term_date)\n",
    "\n",
    "# Calculate moving averages for each window\n",
    "cleanedStockData = cleanedStockData.withColumn(\"5_days_sma\", F.round(F.avg(\"close\").over(short_term_window), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_sma\", F.round(F.avg(\"close\").over(intermediate_term_window), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"50_days_sma\", F.round(F.avg(\"close\").over(fifty_day_window), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"200_days_sma\", F.round(F.avg(\"close\").over(long_term_window), round_to_decimal))\n",
    "\n",
    "# Assuming 'cleanedStockData' is your DataFrame\n",
    "bottom_200_rows = cleanedStockData.select(['ticker_symbol','date', 'close', \"5_days_sma\", \"20_days_sma\", \"50_days_sma\", \"200_days_sma\"]) \\\n",
    "                                  .orderBy(F.desc(\"date\")).limit(200)\n",
    "\n",
    "# Show the bottom 200 rows\n",
    "bottom_200_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Bollinger Bands with SMAs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "round_to_decimal = 2\n",
    "\n",
    "# Assuming you have a SparkSession named spark and your DataFrame is named cleanedStockData\n",
    "intermediate_term_date = 20\n",
    "\n",
    "# Define the window specification\n",
    "bollinger_window = Window().orderBy(F.desc(\"date\")).rowsBetween(0, intermediate_term_date - 1)  # Adjust the window size as needed\n",
    "\n",
    "# Calculate the 20-day simple moving average (SMA)\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_sma\", F.round(F.avg(\"close\").over(bollinger_window), round_to_decimal))\n",
    "\n",
    "# Calculate the standard deviation of the close prices over the same window\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_stddev\", F.round(F.stddev(\"close\").over(bollinger_window), round_to_decimal))\n",
    "\n",
    "# Calculate the upper and lower Bollinger Bands\n",
    "cleanedStockData = cleanedStockData.withColumn(\"upper_band\", F.round(F.col(\"20_days_sma\") + 2 * F.col(\"20_days_stddev\"), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"lower_band\", F.round(F.col(\"20_days_sma\") - 2 * F.col(\"20_days_stddev\"), round_to_decimal))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "cleanedStockData.select(\"date\", \"close\", \"20_days_sma\", \"upper_band\", \"lower_band\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Updated function to calculate exponential moving average\n",
    "def calculate_ema(data, alpha):\n",
    "    ema = data[0]  # Set the initial value as the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        ema = alpha * data[i] + (1 - alpha) * ema\n",
    "    \n",
    "    return ema\n",
    "\n",
    "# Convert the calculate_ema_udf function to a Spark UDF\n",
    "calculate_ema_udf = func.udf(lambda data, alpha: float(calculate_ema(data, alpha)), FloatType())\n",
    "\n",
    "# Define the periods and alpha values\n",
    "short_term_period = 5\n",
    "intermediate_term_period = 20\n",
    "fifty_day_period = 50\n",
    "long_term_period = 200\n",
    "alpha_short = 2 / (short_term_period + 1)\n",
    "alpha_intermediate = 2 / (intermediate_term_period + 1)\n",
    "alpha_fifty = 2 / (fifty_day_period + 1)\n",
    "alpha_long = 2 / (long_term_period + 1)\n",
    "\n",
    "round_to_decimal = 2\n",
    "\n",
    "# Apply the UDF to calculate EMAs\n",
    "cleanedStockData = cleanedStockData.withColumn(\"5_days_ema\", func.round(calculate_ema_udf(func.collect_list(\"close\"). \\\n",
    "                                                                               over(Window().orderBy(func.desc(\"date\")). \\\n",
    "                                                                                    rowsBetween(0, short_term_period - 1)), func.lit(alpha_short)), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_ema\", func.round(calculate_ema_udf(func.collect_list(\"close\").over(Window().orderBy(func.desc(\"date\")). \\\n",
    "                                                                                                                rowsBetween(0, intermediate_term_period - 1)), func.lit(alpha_intermediate)), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"50_days_ema\", func.round(calculate_ema_udf(func.collect_list(\"close\").over(Window().orderBy(func.desc(\"date\")). \\\n",
    "                                                                                                                rowsBetween(0, fifty_day_period - 1)), func.lit(alpha_fifty)), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"200_days_ema\", func.round(calculate_ema_udf(func.collect_list(\"close\").over(Window().orderBy(func.desc(\"date\")). \\\n",
    "                                                                                                                 rowsBetween(0, long_term_period - 1)), func.lit(alpha_long)), round_to_decimal))\n",
    "\n",
    "# Show the result\n",
    "cleanedStockData.select(['date', 'close', '5_days_ema', '20_days_ema', '50_days_ema', '200_days_ema']).orderBy(func.desc(\"date\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Bollinger Bands with EMAs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Updated function to calculate exponential moving average\n",
    "def calculate_ema(data, alpha):\n",
    "    ema = data[0]  # Set the initial value as the first data point\n",
    "    for i in range(1, len(data)):\n",
    "        ema = alpha * data[i] + (1 - alpha) * ema\n",
    "    \n",
    "    return ema\n",
    "\n",
    "# Convert the calculate_ema_udf function to a Spark UDF\n",
    "calculate_ema_udf = F.udf(lambda data, alpha: float(calculate_ema(data, alpha)), FloatType())\n",
    "\n",
    "\n",
    "round_to_decimal = 2\n",
    "\n",
    "# Assuming you have a SparkSession named spark and your DataFrame is named cleanedStockData\n",
    "intermediate_term_date = 20\n",
    "\n",
    "# alpha value for 20 days\n",
    "alpha_intermediate = 2 / (intermediate_term_period + 1)\n",
    "\n",
    "# Define the window specification\n",
    "bollinger_window = Window().orderBy(F.desc(\"date\")).rowsBetween(0, intermediate_term_date - 1)  # Adjust the window size as needed\n",
    "\n",
    "# Calculate the 20-day Exponential moving average (SMA)\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(Window().orderBy(F.desc(\"date\")). \\\n",
    "                                                                                                                rowsBetween(0, intermediate_term_period - 1)), func.lit(alpha_intermediate)), round_to_decimal))\n",
    "\n",
    "# Calculate the standard deviation of the close prices over the same window\n",
    "cleanedStockData = cleanedStockData.withColumn(\"20_days_stddev\", F.round(F.stddev(\"close\").over(bollinger_window), round_to_decimal))\n",
    "\n",
    "# Calculate the upper and lower Bollinger Bands\n",
    "cleanedStockData = cleanedStockData.withColumn(\"upper_band\", F.round(F.col(\"20_days_ema\") + 2 * F.col(\"20_days_stddev\"), round_to_decimal))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"lower_band\", F.round(F.col(\"20_days_ema\") - 2 * F.col(\"20_days_stddev\"), round_to_decimal))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "cleanedStockData.select(\"date\", \"close\", \"20_days_ema\", \"upper_band\", \"lower_band\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming cleanedStockData is your DataFrame\n",
    "# Add a unique identifier column \"ID\"\n",
    "cleanedStockData = cleanedStockData.withColumn(\"ID\", F.monotonically_increasing_id())\n",
    "\n",
    "# Define the window specification\n",
    "partition_cols = [\"ID\"]\n",
    "rsi_window = Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, 13)  # Assuming a 14-day RSI, adjust as needed\n",
    "\n",
    "# Calculate lag without window specification\n",
    "cleanedStockData = cleanedStockData.withColumn(\"lag_close\", F.lag(\"close\").over(Window().orderBy(\"ID\")))\n",
    "cleanedStockData = cleanedStockData.withColumn(\"price_change\", F.when(F.isnull(F.col(\"lag_close\")), 0).otherwise(F.col(\"close\") - F.col(\"lag_close\")))\n",
    "\n",
    "# Calculate the daily gains and losses\n",
    "gains = F.when(F.col(\"price_change\") > 0, F.col(\"price_change\")).otherwise(0)\n",
    "losses = F.when(F.col(\"price_change\") < 0, F.abs(F.col(\"price_change\"))).otherwise(0)\n",
    "\n",
    "# Calculate the average gains and losses over the window\n",
    "avg_gains = F.avg(gains).over(rsi_window)\n",
    "avg_losses = F.avg(losses).over(rsi_window)\n",
    "\n",
    "# Calculate the relative strength (RS)\n",
    "rs = avg_gains / avg_losses\n",
    "\n",
    "# Calculate the RSI\n",
    "rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Add the RSI column to the DataFrame\n",
    "cleanedStockData = cleanedStockData.withColumn(\"rsi_14\", F.when(F.isnull(rsi), 0).otherwise(rsi))\n",
    "\n",
    "# Filter for the latest 200 days\n",
    "latest_200_days = cleanedStockData.orderBy(F.desc(\"date\")).limit(200)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "latest_200_days.select(\"ID\", \"date\", \"close\", \"rsi_14\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
