{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType, ArrayType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/michaelwoan/data-engineering-projects/stock-market-analyzer/data-processing/batch_data_clearning.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/michaelwoan/data-engineering-projects/stock-market-analyzer/data-processing/batch_data_clearning.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39mstop()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/michaelwoan/data-engineering-projects/stock-market-analyzer/data-processing/batch_data_clearning.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mStockDataCleaning\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/michaelwoan/data-engineering-projects/stock-market-analyzer/data-processing/batch_data_clearning.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.executor.memory\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m4g\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "SparkSession.stop()\n",
    "spark = SparkSession.builder.appName(\"StockDataCleaning\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"transaction_id\", StringType(), True), \\\n",
    "    StructField(\"stock_id\", StringType(), True), \\\n",
    "    StructField(\"ticker_symbol\", StringType(), True), \\\n",
    "    StructField(\"date\", DateType(), True), \\\n",
    "    StructField(\"low\", FloatType(), True), \\\n",
    "    StructField(\"open\", FloatType(), True), \\\n",
    "    StructField(\"high\", FloatType(), True), \\\n",
    "    StructField(\"volume\", IntegerType(), True), \\\n",
    "    StructField(\"close\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockData = spark.read.schema(schema).csv(\"./stocks-data/combined-stocks-data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_values = stockData.select([func.sum(func.col(c).isNull().cast(\"int\")).alias(c + '_missing') for c in stockData.columns]).collect()\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "for row in missing_values[0].asDict():\n",
    "    print(f\"{row}: {missing_values[0][row]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'volume' is missing\n",
    "missing_volume_rows = stockData.filter(stockData['volume'].isNull())\n",
    "\n",
    "# Show the missing values\n",
    "missing_volume_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'volume' column with 0\n",
    "stockData = stockData.na.fill(0, subset=['volume'])\n",
    "\n",
    "# Show the DataFrame after filling missing values\n",
    "stockData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows based on 'date' and 'close' columns\n",
    "cleanedStockData = stockData.dropDuplicates(['date', 'close'])\n",
    "\n",
    "# Check for duplicate values in the 'date' column again\n",
    "duplicate_rows = cleanedStockData.groupBy('date', 'close').count().filter('count > 1')\n",
    "\n",
    "# Show the duplicate dates and close prices, if any\n",
    "if duplicate_rows.count() > 0:\n",
    "    print(\"Duplicate dates and close prices found after deduplication:\")\n",
    "    duplicate_rows.show()\n",
    "else:\n",
    "    print(\"No duplicate dates and close prices found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedStockData.orderBy(func.desc(\"date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data Moving Averages Analysis\n",
    "\n",
    "This PySpark script calculates Simple Moving Averages (SMA) and Exponential Moving Averages (EMA) for different periods on stock data. The analysis includes importing libraries, defining functions, setting parameters, and displaying the results.\n",
    "\n",
    "- **Moving Averages:**\n",
    "  - Simple Moving Averages (SMA) for periods: 5, 20, 50, 200.\n",
    "  - Exponential Moving Averages (EMA) with corresponding alpha values.\n",
    "\n",
    "- **Data Manipulation:**\n",
    "  - Utilizes PySpark functions and windows for efficient data processing.\n",
    "\n",
    "- **Result Display:**\n",
    "  - Presents the DataFrame with date, close price, SMAs, and EMAs in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_to_decimal = 2\n",
    "\n",
    "def calculate_ema(data, alpha):\n",
    "    ema = data[0]\n",
    "    for i in range(1, len(data)):\n",
    "        ema = alpha * data[i] + (1 - alpha) * ema\n",
    "    return ema\n",
    "\n",
    "calculate_ema_udf = F.udf(lambda data, alpha: float(calculate_ema(data, alpha)), FloatType())\n",
    "\n",
    "periods = [5, 20, 50, 200]\n",
    "alpha_values = [2 / (p + 1) for p in periods]\n",
    "\n",
    "partition_cols = [\"stock_id\", \"ticker_symbol\"]\n",
    "\n",
    "windows = [Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, p - 1) for p in periods]\n",
    "\n",
    "# Calculate simple moving averages\n",
    "for p in periods:\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{p}_days_sma\", F.round(F.avg(\"close\").over(windows[periods.index(p)]), 2))\n",
    "\n",
    "# Calculate exponential moving averages using UDF\n",
    "for p, alpha in zip(periods, alpha_values):\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{p}_days_ema\", F.round(calculate_ema_udf(F.collect_list(\"close\").over(windows[periods.index(p)]), F.lit(alpha)), round_to_decimal))\n",
    "\n",
    "# Show the result\n",
    "cleanedStockData.select(['date', 'close'] + [f\"{p}_days_sma\" for p in periods] + [f\"{p}_days_ema\" for p in periods]).orderBy(F.desc(\"date\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bollinger Bands Calculation Explanation\n",
    "\n",
    "This Jupiter Notebook cell performs the computation of Bollinger Bands on stock data for volatility analysis. The breakdown includes critical steps and considerations:\n",
    "\n",
    "- **Decimal Rounding:**\n",
    "  - All numerical values are rounded to two decimal places for consistency and readability.\n",
    "\n",
    "- **Bollinger Bands Periods:**\n",
    "  - The Bollinger Bands are computed for four distinct periods: 5, 20, 50, and 200 days, providing insights into short-term and long-term volatility.\n",
    "\n",
    "- **Partitioning for Accuracy:**\n",
    "  - The data is partitioned by \"stock_id\" and \"ticker_symbol\" to ensure accurate calculations for individual stocks. This is crucial for meaningful stock market analysis.\n",
    "\n",
    "- **Reuse of Exponential Moving Averages (EMAs):**\n",
    "  - Existing EMA values, previously calculated, are reused in the Bollinger Bands computation. This approach optimizes computational efficiency and maintains consistency with prior analyses.\n",
    "\n",
    "- **Upper and Lower Band Calculation:**\n",
    "  - The upper and lower bands are determined by adding and subtracting twice the standard deviation of closing prices from the corresponding EMAs. This methodology aligns with the standard Bollinger Bands formula.\n",
    "\n",
    "- **Result Presentation:**\n",
    "  - The final DataFrame includes the date, close price, upper bands, and lower bands for each specified period, providing a comprehensive view of the stock's volatility.\n",
    "\n",
    "This code enhances the dataset with Bollinger Bands, aiding in the identification of potential market trends and volatility patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of decimal places\n",
    "round_to_decimal = 2\n",
    "\n",
    "# Define the Bollinger Bands periods\n",
    "bollinger_periods = [5, 20, 50, 200]\n",
    "\n",
    "partition_cols = [\"stock_id\", \"ticker_symbol\"]\n",
    "\n",
    "# Define the windows for Bollinger Bands\n",
    "windows = [Window().partitionBy(partition_cols).orderBy(F.desc(\"date\")).rowsBetween(0, p - 1) for p in bollinger_periods]\n",
    "\n",
    "# Reuse the existing EMA values for Bollinger Bands\n",
    "for p in bollinger_periods:\n",
    "    upper_band_col = F.col(f\"{p}_days_ema\") + (2 * F.stddev(\"close\").over(windows[bollinger_periods.index(p)]))\n",
    "    lower_band_col = F.col(f\"{p}_days_ema\") - (2 * F.stddev(\"close\").over(windows[bollinger_periods.index(p)]))\n",
    "\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"upper_band_{p}\", F.round(upper_band_col, round_to_decimal))\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"lower_band_{p}\", F.round(lower_band_col, round_to_decimal))\n",
    "\n",
    "# Show the result\n",
    "selected_columns = ['date', 'close'] + [f\"upper_band_{p}\" for p in bollinger_periods] + [f\"lower_band_{p}\" for p in bollinger_periods]\n",
    "cleanedStockData.select(selected_columns).orderBy(F.desc(\"date\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Assuming you already have a SparkSession named spark\n",
    "\n",
    "# Define a function to calculate RSI\n",
    "def calculate_rsi(data, period):\n",
    "    # Calculate daily price changes\n",
    "    price_diff = F.col(\"close\") - F.lag(\"close\", 1).over(Window().orderBy(\"date\"))\n",
    "\n",
    "    # Separate gains and losses\n",
    "    gains = F.when(price_diff > 0, price_diff).otherwise(0)\n",
    "    losses = F.when(price_diff < 0, -price_diff).otherwise(0)\n",
    "\n",
    "    # Calculate average gains and losses over the specified period\n",
    "    avg_gain = F.avg(gains).over(Window().orderBy(\"date\").rowsBetween(-period, Window.currentRow))\n",
    "    avg_loss = F.avg(losses).over(Window().orderBy(\"date\").rowsBetween(-period, Window.currentRow))\n",
    "\n",
    "    # Calculate relative strength (RS)\n",
    "    rs = F.when(avg_loss != 0, avg_gain / avg_loss).otherwise(float(\"inf\"))\n",
    "\n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return F.round(rsi, 2)\n",
    "\n",
    "# Register the RSI UDF\n",
    "calculate_rsi_udf = F.udf(lambda data, period: float(calculate_rsi(data, period)), FloatType())\n",
    "\n",
    "# Define the periods for RSI calculation\n",
    "rsi_periods = [5, 20, 50, 200]\n",
    "\n",
    "# Calculate RSI for each period\n",
    "for period in rsi_periods:\n",
    "    cleanedStockData = cleanedStockData.withColumn(f\"{period}_days_rsi\", calculate_rsi_udf(F.collect_list(\"close\").over(Window().orderBy(\"date\")).alias(\"close_list\"), F.lit(period)))\n",
    "\n",
    "# Show the result\n",
    "cleanedStockData.select(['ticker_symbol', 'date', 'close'] + [f\"{p}_days_rsi\" for p in rsi_periods]).orderBy(F.desc(\"date\")).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
